{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ydZA32Qmu3C"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.40.1 # Use this version and Python 3.10 for stable compatibility\n",
        "!pip install timeseriesviz\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoModelForCausalLM\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from csv import reader, writer\n",
        "from timeseriesviz import plot_numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kog4sDDsQt0j"
      },
      "outputs": [],
      "source": [
        "timeseries_prop = \"prcp(mm/day)\"          # Choose from prcp(mm/day), srad(W/m2), tmax(C), tmin(C), vp(Pa), QObs(mm/d)\n",
        "ckpt_name = f\"CamelsUS-sundial-prcp-ckpt\"\n",
        "run_name = f\"CamelsUS-sundial-prcp\"\n",
        "\n",
        "Restorefromcheckpoint = False       # Restore from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmfP1B76HI1D"
      },
      "outputs": [],
      "source": [
        "def restoreValLocs(ValidationRunName):\n",
        "    InputFileName = APPLDIR + 'Validation' + ValidationRunName\n",
        "    with open(InputFileName, 'r', newline='') as inputfile:\n",
        "        Myreader = reader(inputfile, delimiter=',')\n",
        "        header = next(Myreader)\n",
        "        LocationValidationFraction = np.float32(header[0])\n",
        "        TrainingNloc = np.int32(header[1])\n",
        "        ValidationNloc = np.int32(header[2])\n",
        "\n",
        "        ListofTrainingLocs = np.empty(TrainingNloc, dtype = np.int32)\n",
        "        ListofValidationLocs = np.empty(ValidationNloc,  dtype = np.int32)\n",
        "        nextrow = next(Myreader)\n",
        "        for iloc in range(0, TrainingNloc):\n",
        "            ListofTrainingLocs[iloc] = np.int32(nextrow[iloc])\n",
        "        nextrow = next(Myreader)\n",
        "        for iloc in range(0, ValidationNloc):\n",
        "            ListofValidationLocs[iloc] = np.int32(nextrow[iloc])\n",
        "\n",
        "\n",
        "    return TrainingNloc, ValidationNloc, ListofTrainingLocs, ListofValidationLocs\n",
        "\n",
        "\n",
        "def saveCkpt(iloc, ground_truth, preds, run_name, ckpt_name):\n",
        "    OutputFileName = APPLDIR + ckpt_name\n",
        "    with open(OutputFileName, 'w', newline='') as outputfile:\n",
        "        Mywriter = writer(outputfile, delimiter=',')\n",
        "        Mywriter.writerow([iloc])\n",
        "\n",
        "    np.save(APPLDIR + f\"{run_name}_yin.npy\", ground_truth)\n",
        "    np.save(APPLDIR + f\"{run_name}_FitPredictions.npy\", preds)\n",
        "\n",
        "\n",
        "def restoreCkpt(run_name, ckpt_name):\n",
        "    InputFileName = APPLDIR + ckpt_name\n",
        "    with open(InputFileName, 'r', newline='') as inputfile:\n",
        "        Myreader = reader(inputfile, delimiter=',')\n",
        "        iloc = next(Myreader)[0]\n",
        "\n",
        "    ground_truth = np.load(APPLDIR + f\"{run_name}_yin.npy\", allow_pickle=True)\n",
        "    preds = np.load(APPLDIR + f\"{run_name}_FitPredictions.npy\", allow_pickle=True)\n",
        "\n",
        "    return int(iloc), ground_truth, preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cBDJOivm1Zk"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "APPLDIR = \"/content/gdrive/My Drive/Colab Datasets/Hydrology/\"\n",
        "ValidationRunName = 'Hydrology-CamelsUS-anushka'\n",
        "\n",
        "# Define the file name and file path\n",
        "timeseries_file = 'BasicInputTimeSeries.npy'\n",
        "static_file = 'BasicInputStaticProps.npy'\n",
        "metadata_file = 'metadata.json'\n",
        "\n",
        "timeseries_data = np.load(APPLDIR + timeseries_file, allow_pickle=True)\n",
        "static_data = np.load(APPLDIR + static_file, allow_pickle=True)\n",
        "\n",
        "# Metadata\n",
        "with open(APPLDIR + metadata_file, 'r') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "TrainingNloc, ValidationNloc, ListofTrainingLocs, ListofValidationLocs = restoreValLocs(ValidationRunName)\n",
        "ListofTrainingLocs = [metadata['locs'][i] for i in ListofTrainingLocs]\n",
        "ListofValidationLocs = [metadata['locs'][i] for i in ListofValidationLocs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9YEO4BPqBTD"
      },
      "outputs": [],
      "source": [
        "# Prepare input\n",
        "timeseries_cols = metadata['BasicInputTimeSeries']['fields']\n",
        "static_cols = metadata['BasicInputStaticProps']['fields']\n",
        "\n",
        "# Create the DataFrame\n",
        "df_timeseries = pd.DataFrame(timeseries_data, columns=timeseries_cols)\n",
        "df_static = pd.DataFrame(static_data, columns=static_cols)\n",
        "\n",
        "df_timeseries['ds'] = pd.to_datetime(df_timeseries['Year_Mnth_Day'])\n",
        "df_timeseries['unique_id'] = df_timeseries['basin_id']\n",
        "df_static = df_static.rename(columns={\"gauge_id\": \"basin_id\"})\n",
        "merged_df = pd.merge(df_timeseries, df_static, on='basin_id', how='inner')\n",
        "merged_df.drop(['Year_Mnth_Day', 'basin_id'], axis=1, inplace=True)\n",
        "\n",
        "df_timeseries.drop(['Year_Mnth_Day', 'basin_id'], axis=1, inplace=True)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "for col in df_timeseries.columns:\n",
        "    if col != 'ds' and col !='unique_id':\n",
        "        if col == 'prcp(mm/day)' or col == 'QObs(mm/d)':\n",
        "            df_timeseries[col] = df_timeseries[col].clip(lower=0)\n",
        "            df_timeseries[col] = df_timeseries[col]**(1/3)\n",
        "\n",
        "        df_timeseries[col] = scaler.fit_transform(df_timeseries[[col]])\n",
        "\n",
        "print(df_timeseries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_d4PQlGNm3Iy"
      },
      "outputs": [],
      "source": [
        "# load pretrain model\n",
        "# supports different lookback/forecast lengths\n",
        "model = AutoModelForCausalLM.from_pretrained('thuml/sundial-base-128m', trust_remote_code=True)\n",
        "\n",
        "# perpare input\n",
        "lookback_length = 21\n",
        "Nloc = 671\n",
        "Ndays = 7031\n",
        "\n",
        "# forecasting configurations\n",
        "forecast_length = 1       # forecast the next 288 timestamps\n",
        "num_samples = 1           # generate 1 sample\n",
        "\n",
        "timeseries_cols = timeseries_cols[2:]\n",
        "Nprop = len(timeseries_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXhQlcP08EE5"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "ground_truth = np.zeros((Ndays - lookback_length, ValidationNloc, Nprop))\n",
        "preds = np.zeros((Ndays - lookback_length, ValidationNloc, Nprop))\n",
        "iloc = 0\n",
        "\n",
        "if Restorefromcheckpoint:\n",
        "    iloc, ground_truth, preds = restoreCkpt(run_name, ckpt_name)\n",
        "    iloc += 1   # start evaluating the next unchecked location\n",
        "    print(f\"Restored from checkpoint: iloc={iloc}\")\n",
        "\n",
        "\n",
        "# for each input time series property\n",
        "iprop = timeseries_cols.index(timeseries_prop)\n",
        "print(f\"Starting inferencing on prop {iprop}: {timeseries_prop}\")\n",
        "print(\"=\"*100)\n",
        "\n",
        "# prepare input\n",
        "df = (df_timeseries.set_index(['ds','unique_id']).unstack('unique_id')[timeseries_prop].sort_index())\n",
        "\n",
        "# look at each catchment\n",
        "while iloc < ValidationNloc:\n",
        "    cat_id = ListofValidationLocs[iloc]\n",
        "    # look at each sequence for that catchment\n",
        "    for t in range(Ndays - lookback_length):\n",
        "        normalized_input = df[cat_id][t: t + lookback_length]\n",
        "        lookback = torch.tensor(df[cat_id][t: t + lookback_length]).unsqueeze(0).float()\n",
        "        forecast = model.generate(lookback, max_new_tokens=forecast_length, num_samples=num_samples)\n",
        "        # update preds & label\n",
        "        ground_truth[t, iloc, iprop] = df[cat_id][t + lookback_length]\n",
        "        preds[t, iloc, iprop] = forecast[0][0].float()\n",
        "\n",
        "    saveCkpt(iloc, ground_truth, preds, run_name, ckpt_name)\n",
        "\n",
        "    iloc += 1\n",
        "    print(f\"Location {iloc} saved to checkpoint\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ny4Ug8hpCCuS"
      },
      "outputs": [],
      "source": [
        "# calculate MSE\n",
        "for iprop in range(len(timeseries_cols)):\n",
        "    prop = timeseries_cols[iprop]\n",
        "    mse = np.mean((ground_truth[:, :, iprop] - preds[:, :, iprop])**2)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f\"MSE for {prop}: {mse}\")\n",
        "    print(f\"RMSE for {prop}: {rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot\n",
        "fig, axs = plot_numpy(ground_truth[:, :, iprop], pred[:, :, iprop], f'Sundial {timeseries_prop}', splitsize=6)"
      ],
      "metadata": {
        "id": "rJn8mcomspLc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}